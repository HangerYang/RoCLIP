{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"imagenet30\"\n",
    "model_name = \"NNCLIP_100K_NN_CROSS_s_step\"\n",
    "begin_epoch = 1\n",
    "end_epoch = 70\n",
    "total_poison = []\n",
    "for i in range(begin_epoch, end_epoch):\n",
    "    path = \"template/{}_{}_{}_0.npz\".format(model_name, dataset, str(i))\n",
    "    while True:\n",
    "        try:\n",
    "            np.load(path)\n",
    "            a = np.load(path)\n",
    "            total_poison.append(a['arr_0'])\n",
    "            # print(\"-------------------------------\")\n",
    "            # print(\"model name: \" + model_name)\n",
    "            print(\"epoch: \" + str(i))\n",
    "            print(\"psr: \" + str(a['arr_0']))\n",
    "            print(\"poison class: \" +str(a['arr_3']))\n",
    "            print(\"poison cosine similarity: \" + str(a['arr_4']))\n",
    "            print(\"top three cosine similarity: \" + str(a['arr_5']))\n",
    "            break\n",
    "        except:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanse = np.arange(5.5, 32, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "# def plot_cosine_similarity(cosine_similarity, update_epoch):\n",
    "plt.plot(total_poison)\n",
    "for xc in cleanse:\n",
    "    plt.axvline(x=xc, color = 'k', linestyle = '--')\n",
    "plt.xticks(np.arange(len(total_poison)), np.arange(1, len(total_poison)+1))\n",
    "plt.ylabel('poison ratio')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "try:\n",
    "    np.load(\"CyCLIP/hello.npz\")\n",
    "except:\n",
    "    print(\"hello, world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 7\n",
    "a = np.load(\"/home/hyang/RoCLIP/CyCLIP/representation/1M_inmodal_continue_{}.npz\".format(epoch))\n",
    "clean_idx = a['index'] < 1000000\n",
    "poison_idx = a['index'] >= 1000000\n",
    "distance = np.percentile(a['img_txt'][poison_idx], 95)\n",
    "sum(a['img_txt'][clean_idx] > distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 7\n",
    "a = np.load(\"/home/hyang/RoCLIP/CyCLIP/representation/1M_inmodal_continue_slr_{}.npz\".format(epoch))\n",
    "clean_idx = a['index'] <= 1000000\n",
    "poison_idx = a['index'] > 1000000\n",
    "distance = np.percentile(a['img_txt'][poison_idx], 100)\n",
    "sum(a['img_txt'][clean_idx] > distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 7\n",
    "a = np.load(\"/home/hyang/RoCLIP/CyCLIP/representation/1M_inmodal_continue_{}.npz\".format(epoch))\n",
    "clean_idx = a['index'] < 1000000\n",
    "poison_idx = a['index'] >= 1000000\n",
    "distance = np.percentile(a['img_txt'][poison_idx], 95)\n",
    "sum(a['img_txt'][clean_idx] > distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ntyf_index = a['img_txt'] > distance\n",
    "truth = a['index']\n",
    "sort = np.argsort(truth)\n",
    "sorted_truth = truth[sort]\n",
    "# sorted_nty_index = nty_index[sort]\n",
    "sorted_ntyf_index = ntyf_index[sort]\n",
    "df = pd.read_csv(\"../train_1M_random_poison_1_100.csv\")\n",
    "final_bool_ntyf = np.full(1000500, -1)\n",
    "for i in range(1000500):\n",
    "    if i not in truth:\n",
    "        final_bool_ntyf[i] = True\n",
    "    else:\n",
    "        loc = np.where(sorted_truth == i)[0][0]\n",
    "        final_bool_ntyf[i] = sorted_ntyf_index[loc]\n",
    "df[final_bool_ntyf == 1].to_csv(\"train_1M_random_poison_1_100_100.csv\", index=False, header=[\"caption\", \"path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "poison_per_category = 30\n",
    "# poisoners = ['desk', 'palace', 'necklace', 'balloon', 'pillow', \n",
    "#              'candle', 'pizza', 'umbrella', 'television', \"baseball\", \n",
    "#              \"ice cream\", \"suit\", 'mountain', 'beach', 'plate',\n",
    "#              'orange']\n",
    "poisoners = [\"pizza\", \"baseball\", \"tiger\", \"candle\", \"ice cream\", \"desk\", \"palace\", \"necklace\"]\n",
    "full_poison_range = poison_per_category * len(poisoners)\n",
    "size_of_data = 100000\n",
    "def plot_poison_distribution(file_path, poison_category='full', filter_ratios=[0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]):\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "    mean_similarity = df[1].mean()\n",
    "    orig_len = len(df)\n",
    "\n",
    "    if poison_category == 'full':\n",
    "        condition = df[0] > (size_of_data)\n",
    "    else:\n",
    "        condition = (df[0] >= poison_per_category * poisoners.index(poison_category) \\\n",
    "                        and df[0] < poison_per_category * (poisoners.index(poison_category)+1))\n",
    "    df = df[condition]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n",
    "\n",
    "    n, bins, patches = ax1.hist(df.index.tolist(), bins=50, color='blue', alpha=0.5)\n",
    "    ax1.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    ax1.set_title('Poison Rank Distribution')\n",
    "    ax1.set_xlabel('Poison Rank')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    \n",
    "    comments = []\n",
    "    for ratio in filter_ratios:\n",
    "        unfiltered_poison_num = (df.index < orig_len * ratio).sum()\n",
    "        comments.append('poison num at top %f: %d'%(ratio, unfiltered_poison_num))\n",
    "    comment_x = np.argmax(n)\n",
    "    comment_y = np.max(n) \n",
    "    ax1.text(comment_x, comment_y, '\\n'.join(comments), fontsize=12, ha='center')\n",
    "\n",
    "\n",
    "    ax2.hist(df[1].tolist(), bins=30, color='green', alpha=0.5)\n",
    "    ax2.invert_xaxis()\n",
    "    ax2.set_title('Poison Similarity Distribution')\n",
    "    ax2.set_xlabel('Poison Similarity')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.axvline(mean_similarity, color='red', linestyle='--', label='mean_similarity')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('post_pretraining_analysis/dist_%s_%s.png' \\\n",
    "                %(re.search(r\"/([^/]+).tsv\", file_path).group(1), poison_category))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7,64,1):\n",
    "    plot_poison_distribution(\"indices/NNCLIP_100K_NN_CROSS_s_step_update{}.tsv\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('logs/trial_1M_aug/output.log', 'r')\n",
    "Lines = file1.readlines()\n",
    "\n",
    " \n",
    "count = 0\n",
    "learn_rate_inmodal = []\n",
    "learn_rate_crossmodal = []\n",
    "cross_modal = True\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    a = re.search(\"Learning Rate:\", line)\n",
    "    b = re.search(\"Cross-modal training\", line)\n",
    "    c = re.search(\"In-modal training\", line)\n",
    "    if b:\n",
    "        cross_modal = True\n",
    "    if c:\n",
    "        cross_modal = False\n",
    "    if a and cross_modal:\n",
    "        learn_rate_crossmodal.append(float(line.strip()[-11:]))\n",
    "    elif a:\n",
    "        # print(line)\n",
    "        # print(cross_modal)\n",
    "        learn_rate_inmodal.append(float(line.strip()[-11:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "file1 = open(\"logs/NNCLIP_1M/output.log\", 'r')\n",
    "Lines = file1.readlines()\n",
    "\n",
    " \n",
    "learn_rate = []\n",
    "# learn_rate_crossmodal = []\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    a = re.search(\"Learning Rate:\", line)\n",
    "    if a:\n",
    "        learn_rate.append(float(line.strip()[-11:]))\n",
    "        # print(line.strip()[-11:])\n",
    "plt.plot(learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "file1 = open(\"logs/NNCLIP_100K_NN_CROSS_s_step/output.log\", 'r')\n",
    "Lines = file1.readlines()\n",
    "\n",
    " \n",
    "learn_rate = []\n",
    "# learn_rate_crossmodal = []\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    a = re.search(\"Learning Rate:\", line)\n",
    "    if a:\n",
    "        learn_rate.append(float(line.strip()[-11:]))\n",
    "        # print(line.strip()[-11:])\n",
    "plt.plot(learn_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python -m src.main --name NNCLIP_100K_NN_CROSS --train_data ../train_100K_30.csv --validation_data ../valid_temp.csv --multimodal_warmup 1 --inmodal_warmup 5 --loader_update_freq 4 --filter_ratio 0.2 --epochs 32 --post_lr 5e-6 --update_filter_ratio 0.1 --batch_size 128 --inmodal --memory_bank --memory_bank_size 2048 --index_dir indices --save_index --device_id 6\n",
    "\n",
    "python -m src.main --name NNCLIP_100K_CROSS --train_data ../train_100K_30.csv --validation_data ../valid_temp.csv --multimodal_warmup 1 --inmodal_warmup 5 --loader_update_freq 4 --filter_ratio 0.2 --epochs 32 --post_lr 5e-6 --update_filter_ratio 0.1 --batch_size 128 --inmodal --index_dir indices --save_index --device_id 7\n",
    "\n",
    "python -m src.main --name NNCLIP_100K_NN_CROSS_s_step --train_data ../train_100K_30.csv --validation_data ../valid_temp.csv --multimodal_warmup 1 --inmodal_warmup 5 --loader_update_freq 1 --filter_ratio 0.2 --epochs 64 --post_lr 5e-6 --update_filter_ratio 0.01 --batch_size 128 --inmodal --memory_bank --memory_bank_size 2048 --index_dir indices --save_index --device_id 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class op(object):\n",
    "    def __init__(self, epochs, inmodal_warmup, multimodal_warmup, loader_update_freq, update_filter_ratio, filter_ratio):\n",
    "            self.epochs = epochs\n",
    "            self.inmodal_warmup = inmodal_warmup\n",
    "            self.multimodal_warmup = multimodal_warmup\n",
    "            self.loader_update_freq = loader_update_freq\n",
    "            self.update_filter_ratio = update_filter_ratio\n",
    "            self.filter_ratio = filter_ratio\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = op(64, 5, 1, 1, 0.01, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcualte_num_batches(options, num_batches):\n",
    "    update_epoch = options.epochs - options.inmodal_warmup - options.multimodal_warmup\n",
    "    num_update = update_epoch // options.loader_update_freq\n",
    "    left_off_epoch = update_epoch % options.loader_update_freq\n",
    "    total_step = 0\n",
    "    for i in range(num_update):\n",
    "        total_step = total_step + (i * options.update_filter_ratio + options.filter_ratio) * options.loader_update_freq * num_batches\n",
    "    total_step =  total_step + ((i+1) * options.update_filter_ratio + options.filter_ratio) * num_batches * left_off_epoch\n",
    "    return total_step\n",
    "\n",
    "\n",
    "def cosine_scheduler_mock(base_lr, post_lr, num_warmup_steps, total_steps):\n",
    "    def _scheduler(current_step, lr_adjust=False):\n",
    "        if lr_adjust:\n",
    "            lr = post_lr\n",
    "        elif(current_step < num_warmup_steps):\n",
    "            lr = base_lr * (current_step + 1) / num_warmup_steps\n",
    "        else:\n",
    "            n = current_step - num_warmup_steps\n",
    "            d = total_steps - num_warmup_steps\n",
    "            lr = 0.5 * (1 + np.cos(np.pi * n / d)) * base_lr\n",
    "        return lr, current_step\n",
    "    return _scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(7,64))\n",
    "# num_batches = [293] * 5 + [488] * 5 + [684] * 5 + [879] * 5 + [1075] * 5 + [1270]\n",
    "# num_batches = [1075] * 26\n",
    "num_batches = list(range(585, 3200, 40))\n",
    "total = calcualte_num_batches(options, 1954)\n",
    "train_num_batches = total // (options.epochs - options.inmodal_warmup - options.multimodal_warmup)\n",
    "\n",
    "# total = 1954 * 32\n",
    "scheduler = cosine_scheduler_mock(base_lr = 0.0005, post_lr = 5e-06, num_warmup_steps = 8000, total_steps = total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lr = []\n",
    "total_step = []\n",
    "for i in epochs:\n",
    "    batch_size = num_batches[i-7]\n",
    "    for j in range(batch_size):\n",
    "        step = batch_size * (i - 6) + j\n",
    "        lr, current_step = scheduler(step)\n",
    "        total_lr.append(lr) \n",
    "        total_step.append(current_step) \n",
    "\n",
    "plt.plot(total_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
    "    \"\"\"\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        first_cycle_steps (int): First cycle step size.\n",
    "        cycle_mult(float): Cycle steps magnification. Default: -1.\n",
    "        max_lr(float): First cycle's max learning rate. Default: 0.1.\n",
    "        min_lr(float): Min learning rate. Default: 0.001.\n",
    "        warmup_steps(int): Linear warmup step size. Default: 0.\n",
    "        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 optimizer : torch.optim.Optimizer,\n",
    "                 first_cycle_steps : int,\n",
    "                 cycle_mult : float = 1.,\n",
    "                 max_lr : float = 0.1,\n",
    "                 min_lr : float = 0.001,\n",
    "                 warmup_steps : int = 0,\n",
    "                 gamma : float = 1.,\n",
    "                 last_epoch : int = -1\n",
    "        ):\n",
    "        assert warmup_steps < first_cycle_steps\n",
    "        \n",
    "        self.first_cycle_steps = first_cycle_steps # first cycle step size\n",
    "        self.cycle_mult = cycle_mult # cycle steps magnification\n",
    "        self.base_max_lr = max_lr # first max learning rate\n",
    "        self.max_lr = max_lr # max learning rate in the current cycle\n",
    "        self.min_lr = min_lr # min learning rate\n",
    "        self.warmup_steps = warmup_steps # warmup step size\n",
    "        self.gamma = gamma # decrease rate of max learning rate by cycle\n",
    "        \n",
    "        self.cur_cycle_steps = first_cycle_steps # first cycle step size\n",
    "        self.cycle = 0 # cycle count\n",
    "        self.step_in_cycle = last_epoch # step size of the current cycle\n",
    "        \n",
    "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
    "        \n",
    "        # set learning rate min_lr\n",
    "        self.init_lr()\n",
    "    \n",
    "    def init_lr(self):\n",
    "        self.base_lrs = []\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.min_lr\n",
    "            self.base_lrs.append(self.min_lr)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.step_in_cycle == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.step_in_cycle < self.warmup_steps:\n",
    "            return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.max_lr - base_lr) \\\n",
    "                    * (1 + math.cos(math.pi * (self.step_in_cycle-self.warmup_steps) \\\n",
    "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.step_in_cycle = self.step_in_cycle + 1\n",
    "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
    "                self.cycle += 1\n",
    "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n",
    "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
    "        else:\n",
    "            if epoch >= self.first_cycle_steps:\n",
    "                if self.cycle_mult == 1.:\n",
    "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
    "                    self.cycle = epoch // self.first_cycle_steps\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
    "                    self.cycle = n\n",
    "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
    "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\n",
    "            else:\n",
    "                self.cur_cycle_steps = self.first_cycle_steps\n",
    "                self.step_in_cycle = epoch\n",
    "                \n",
    "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyang/deadclip/CyCLIP/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(a, b):\n",
    "    return list(set(a) & set(b))\n",
    "\n",
    "def diff(a,b):\n",
    "    return list(set(a) - set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"/home/hyang/NNCLIP/CyCLIP/indices/NNCLIP_1M_update{}.tsv\".format(i)\n",
    "df_origin = pd.read_csv(fp, sep='\\t', header=None, index_col = [0])\n",
    "idx = [521591,974537,850994]\n",
    "df_origin.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "update file\n",
    "Epoch 8 Poison Number: 28\n",
    "New Evaluation Poison Number: 28\n",
    "How accurate is the set: 0.9235486610711431\n",
    "Current Evaluation Threshold:0.5\n",
    "\n",
    "Epoch 9 Poison Number: 9\n",
    "New Evaluation Poison Number: 10\n",
    "How accurate is the set: 0.8966879242954124\n",
    "Current Evaluation Threshold:0.51\n",
    "\n",
    "Epoch 10 Poison Number: 16\n",
    "New Evaluation Poison Number: 13\n",
    "How accurate is the set: 0.835303979038991\n",
    "Current Evaluation Threshold:0.52\n",
    "\n",
    "Epoch 11 Poison Number: 13\n",
    "New Evaluation Poison Number: 17\n",
    "How accurate is the set: 0.8152688375615297\n",
    "Current Evaluation Threshold:0.53\n",
    "\n",
    "Epoch 12 Poison Number: 3\n",
    "New Evaluation Poison Number: 8\n",
    "How accurate is the set: 0.7882094324540367\n",
    "Current Evaluation Threshold:0.54\n",
    "\n",
    "update file\n",
    "Epoch 13 Poison Number: 1\n",
    "New Evaluation Poison Number: 1\n",
    "How accurate is the set: 0.9128363975486278\n",
    "Current Evaluation Threshold:0.5\n",
    "\n",
    "Epoch 14 Poison Number: 2\n",
    "New Evaluation Poison Number: 2\n",
    "How accurate is the set: 0.8939484775815711\n",
    "Current Evaluation Threshold:0.51\n",
    "\n",
    "Epoch 15 Poison Number: 1\n",
    "New Evaluation Poison Number: 3\n",
    "How accurate is the set: 0.8770717211957443\n",
    "Current Evaluation Threshold:0.52\n",
    "\n",
    "Epoch 16 Poison Number: 1\n",
    "New Evaluation Poison Number: 1\n",
    "How accurate is the set: 0.8609778843591793\n",
    "Current Evaluation Threshold:0.53\n",
    "\n",
    "Epoch 17 Poison Number: 0\n",
    "New Evaluation Poison Number: 0\n",
    "How accurate is the set: 0.8448481215027978\n",
    "Current Evaluation Threshold:0.54\n",
    "\n",
    "update file\n",
    "Epoch 18 Poison Number: 2\n",
    "New Evaluation Poison Number: 1\n",
    "How accurate is the set: 0.8861833610035049\n",
    "Current Evaluation Threshold:0.5\n",
    "\n",
    "Epoch 19 Poison Number: 0\n",
    "New Evaluation Poison Number: 0\n",
    "How accurate is the set: 0.8757956597684815\n",
    "Current Evaluation Threshold:0.51\n",
    "\n",
    "Epoch 20 Poison Number: 0\n",
    "New Evaluation Poison Number: 0\n",
    "How accurate is the set: 0.866121388603403\n",
    "Current Evaluation Threshold:0.52\n",
    "\n",
    "Epoch 21 Poison Number: 0\n",
    "New Evaluation Poison Number: 0\n",
    "How accurate is the set: 0.8664241020976322\n",
    "Current Evaluation Threshold:0.53\n",
    "\n",
    "Epoch 22 Poison Number: 0\n",
    "New Evaluation Poison Number: 0\n",
    "How accurate is the set: 0.8579602984279243\n",
    "Current Evaluation Threshold:0.54\n",
    "\n",
    "update file\n",
    "Epoch 23 Poison Number: 0\n",
    "New Evaluation Poison Number: 0\n",
    "How accurate is the set: 0.8742038627162786\n",
    "Current Evaluation Threshold:0.5\n",
    "\n",
    "Epoch 24 Poison Number: 0\n",
    "New Evaluation Poison Number: 0\n",
    "How accurate is the set: 0.8665442645883293\n",
    "Current Evaluation Threshold:0.51\n",
    "\n",
    "Epoch 25 Poison Number: 0\n",
    "New Evaluation Poison Number: 0\n",
    "How accurate is the set: 0.8628486993698962\n",
    "Current Evaluation Threshold:0.52\n",
    "\n",
    "Epoch 26 Poison Number: 0\n",
    "New Evaluation Poison Number: 0\n",
    "How accurate is the set: 0.860467098283427\n",
    "Current Evaluation Threshold:0.53\n",
    "\n",
    "Epoch 27 Poison Number: 0\n",
    "New Evaluation Poison Number: 0\n",
    "How accurate is the set: 0.8605487038940276\n",
    "Current Evaluation Threshold:0.54\n",
    "\n",
    "update file\n",
    "Epoch 28 Poison Number: 0\n",
    "New Evaluation Poison Number: 0\n",
    "How accurate is the set: 0.8555988542499334\n",
    "Current Evaluation Threshold:0.5\n",
    "\n",
    "Epoch 29 Poison Number: 0\n",
    "New Evaluation Poison Number: 0\n",
    "How accurate is the set: 0.8505357875861473\n",
    "Current Evaluation Threshold:0.51\n",
    "\n",
    "Epoch 30 Poison Number: 0\n",
    "New Evaluation Poison Number: 0\n",
    "How accurate is the set: 0.8477691530985738\n",
    "Current Evaluation Threshold:0.52\n",
    "\n",
    "Epoch 31 Poison Number: 0\n",
    "New Evaluation Poison Number: 0\n",
    "How accurate is the set: 0.8446217385066306\n",
    "Current Evaluation Threshold:0.53\n",
    "\n",
    "Epoch 32 Poison Number: 1\n",
    "New Evaluation Poison Number: 0\n",
    "How accurate is the set: 0.845790867306155\n",
    "Current Evaluation Threshold:0.54\n",
    "\n",
    "update file\n",
    "Epoch 33 Poison Number: 1\n",
    "New Evaluation Poison Number: 1\n",
    "How accurate is the set: 0.846271763077343\n",
    "Current Evaluation Threshold:0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([521591, 974537, 850994], dtype='int64', name=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_origin[1][idx].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = 0.15\n",
    "x = 0.5\n",
    "for i in range(7,64):    \n",
    "    if (i - 7) % 5 == 0:\n",
    "        fp = \"/home/hyang/NNCLIP/CyCLIP/indices/NNCLIP_1M_update{}.tsv\".format(i)\n",
    "        df_origin = pd.read_csv(fp, sep='\\t', header=None, index_col = [0])\n",
    "        indices_2 = df_origin.index.tolist()\n",
    "        x = x - 0.05\n",
    "        x = max(x,0.2)\n",
    "        \n",
    "    file_path = \"/home/hyang/NNCLIP/CyCLIP/indices/NNCLIP_1M_update{}.tsv\".format(i+1)\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None, index_col = [0])\n",
    "    indices = np.array(df.index.tolist())\n",
    "    length = len(indices)\n",
    "    ratio = int(length * (default + (i - 6) * 0.01))\n",
    "    ratio_2 = int(length * (default + (i - 7) * 0.01 + x) )\n",
    "    \n",
    "    old_search_range = indices[:ratio]\n",
    "    \n",
    "    new_search_range = indices_2[:ratio_2]\n",
    "\n",
    "    sorted_idx = torch.argsort(torch.tensor(indices_2)).numpy()\n",
    "    sorted_new_cs = np.array(df_origin.values.flatten())[sorted_idx]\n",
    "    sorted_indices = indices_2[sorted_idx]\n",
    "    \n",
    "    what_we_used_idx = sorted_indices[c]\n",
    "    what_we_used_sim = sorted_new_cs[what_we_used_idx]\n",
    "    \n",
    "    real_new_sorted_descending_cs_idx = torch.argsort(torch.tensor(what_we_used_sim), descending=True).numpy()\n",
    "    sample_indices = what_we_used_idx[real_new_sorted_descending_cs_idx]\n",
    "    d = sample_indices[:ratio]\n",
    "\n",
    "    intersect_larger = intersection(b,d)\n",
    "\n",
    "    print(\"Epoch {} Poison Number: {}\".format(i+1, sum(np.array(b) > 1000000)))\n",
    "    print(\"New Evaluation Poison Number: {}\".format(sum(np.array(d) > 1000000)))\n",
    "    print(\"How accurate is the set: {}\\n\".format(len(intersect_larger) / len(b)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update file\n"
     ]
    }
   ],
   "source": [
    "# this is the initial evaluation on cosine similarity. We will judge which data we will evaluate in the \n",
    "# new epoch\n",
    "default_threshold = 0.15\n",
    "additional_threshold = 0.35\n",
    "for i in range(7,33):\n",
    "    if (i - 7) % 5 == 0:\n",
    "        filter_pointer = \"/home/hyang/NNCLIP/CyCLIP/indices/NNCLIP_1M_update{}.tsv\".format(i)\n",
    "        df = pd.read_csv(filter_pointer, sep='\\t', header=None, index_col = [0])\n",
    "        idx_pointer = df.index.tolist()\n",
    "        print(\"update file\")\n",
    "        if i - 7 > 0:\n",
    "            additional_threshold = additional_threshold - 0.05\n",
    "            additional_threshold = max(additional_threshold, 0.1)\n",
    "    # this is the ground truth updated cosine similarity. \n",
    "    filter_truth = \"/home/hyang/NNCLIP/CyCLIP/indices/NNCLIP_1M_update{}.tsv\".format(i+1)\n",
    "    df_update = pd.read_csv(filter_truth, sep='\\t', header=None, index_col = [0])\n",
    "    idx_update = np.array(df_update.index.tolist())\n",
    "\n",
    "    # total number of data\n",
    "    data_size = len(idx_update)\n",
    "\n",
    "    # ground_true ratio: if we update every epoch, the ratio for search\n",
    "    ratio = int(data_size * (default_threshold + (i - 6) * 0.01))\n",
    "\n",
    "    # ground truth idx\n",
    "    ground_truth_idx = idx_update[:ratio]\n",
    "\n",
    "    # the range if we want to rely on the initial pointer. \n",
    "    ratio_mock = int(data_size * (default_threshold + (i - 7) * 0.01 + additional_threshold))\n",
    "\n",
    "    # the idx we need to re-evaluate to determine what should we choose for the new epoch.\n",
    "\n",
    "    research_idx = idx_pointer[:ratio_mock]\n",
    "\n",
    "\n",
    "    research_df = df_update[1][research_idx]\n",
    "    research_idx = research_df.index\n",
    "    research_value = research_df.values\n",
    "\n",
    "    sorted_research_idx = torch.argsort(torch.tensor(research_value), descending=True)\n",
    "    new_research_idx = research_idx[sorted_research_idx].tolist()[:ratio]\n",
    "    # new_research_value = research_value[sorted_research_idx][:ratio]\n",
    "    # break\n",
    "    # df[1][research_idx] = research_value\n",
    "    # df = df.sort_values(by=[1], ascending=False)\n",
    "    # new_idx_pointer = df.index.tolist()\n",
    "\n",
    "\n",
    "\n",
    "    final_intersect = intersection(ground_truth_idx,new_research_idx)\n",
    "\n",
    "    print(\"Epoch {} Poison Number: {}\".format(i+1, sum(np.array(ground_truth_idx) > 1000000)))\n",
    "    print(\"New Evaluation Poison Number: {}\".format(sum(np.array(new_research_idx) > 1000000)))\n",
    "    print(\"How accurate is the set: {}\".format(len(final_intersect) / len(ground_truth_idx)))\n",
    "    print(\"Current Evaluation Threshold:{}\\n\".format(default_threshold + (i - 7) * 0.01 + additional_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1][research_idx] = research_value\n",
    "df = df.sort_values(by=[1], ascending=False)\n",
    "new_idx_pointer = df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = idx_pointer[:ratio_mock]\n",
    "b = new_idx_pointer[:ratio_mock]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9878697042366107"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(intersection(a,b)) / len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python -m src.main --name NNCLIP_1M_0.02 --train_data ../train_1M_100.csv --validation_data ../valid_temp.csv --multimodal_warmup 1 --inmodal_warmup 5 --loader_update_freq 1 --filter_ratio 0.15 --epochs 32 --filter_lr 5e-6 --update_filter_ratio 0.02 --batch_size 256 --memory_bank --memory_bank_size 21840 --index_dir indices --device_ids 0 1 --distributed --num_warmup_steps 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python -m src.main --name NNCLIP_1M_500 --train_data ../train_1M_500.csv --validation_data ../valid_temp.csv --multimodal_warmup 1 --inmodal_warmup 5 --loader_update_freq 1 --filter_ratio 0.15 --epochs 32 --filter_lr 5e-6 --update_filter_ratio 0.01 --batch_size 256 --memory_bank --memory_bank_size 21840 --index_dir indices --device_ids 2 3 --distributed --num_warmup_steps 8000 --distributed_init_method tcp://127.0.0.1:5431"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0b61b801bee499609bf75262e7f96988907fc8b11da351027b342a461b231a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
